{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chat_bot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/himanshushakyawar/BasicLibraries/blob/main/chat_bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjvd6GR4G6gB"
      },
      "outputs": [],
      "source": [
        "# history of chatbots\n",
        "# history of chatbots dates back to 1966 when a computer program called ELIZA was invented by Weizenbaum. \n",
        "# he wrote 200 lines of code and he was a psychotherapist\n",
        "# spacy, nltk, tflearn.."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NLP :- natural language processing\n",
        "NLP is way computers analyse, understand, and derive meanings from human language in a smart and useful way\n",
        "by utilizing NLP, developers can organise and structure knowledge to perform certain tasks such as automatic summariation, translation,\n",
        "NER (name entity recognition), relationship extraction, sentiment analysis, speech recognition, and topic segmentation"
      ],
      "metadata": {
        "id": "UnCMiI19IftX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import random\n",
        "import string\n",
        "import warnings\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "MNgXi7cYJ9gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK (natural language toolkit) is a leading platform for building python programs to work with human language"
      ],
      "metadata": {
        "id": "zXSo2779KqGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8a2siYrLFic",
        "outputId": "8000ec8d-4433-4593-e008-fe454a3914f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('popular', quiet=True)\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbOcBo0MLHcN",
        "outputId": "21ca7cc1-f484-43c4-d05b-14d3e3c95b71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from nltk.parse.chart import LeftCornerChartParser\n",
        "# The main issue with the text file is that all the text is in string format. but ML algorithms need numerical features. before \n",
        "# we start any NLP project we need to do basic pre processing. \n",
        "# 1. convert text from any case to lower case so that same words will not be treated differently\n",
        "# 2. tokenization - converting sentenses to individual words.\n",
        "# 3. remove noise - removing anthing which is not a word or a LeftCornerChartParser\n",
        "# 4. remove stop words - common words with little value like is as then if . , \n",
        "# 5. stemming - converting word to base words. eg go gone going has a base word go\n",
        "# 6. Lemmatization - stemming can often create non existent words but lemmatization gives you exact word."
      ],
      "metadata": {
        "id": "YkNYl9-tMaPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1st step\n",
        "f = open('/content/chatbot.txt')\n",
        "raw=f.read()\n",
        "raw=raw.lower()"
      ],
      "metadata": {
        "id": "QigHK9xFLqU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 step - converting paragraphs to sentence and then to tokens\n",
        "set_tokens = nltk.sent_tokenize(raw)\n",
        "word_tokens = nltk.word_tokenize(raw)\n",
        "len(word_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZ9PHk0iN5Uh",
        "outputId": "809c4322-40cc-4ab5-c1b5-5e2585e72ebb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2188"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3rd step is preprocessing\n",
        "# we will define function called LemTokens which will take input as token and return normalised tokens\n",
        "lemmer = nltk.stem.WordNetLemmatizer() # gives us semantically - organised dictionary pf english words\n",
        "\n",
        "def LemTokens(tokens):\n",
        "  return [lemmer.lemmatize(t) for t in tokens ]\n",
        "\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation) # this will remove &, * ()#@  any special char\n",
        "\n",
        "def LemNormalize(text):\n",
        "  return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
        "\n",
        "\n",
        "LemNormalize(\"i am going and gone **& to DEL*I\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJl8cwKuONr2",
        "outputId": "dd55dcd8-3c4e-482d-a347-b41b7483a0fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'am', 'going', 'and', 'gone', 'to', 'deli']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import nltk\n",
        "# from nltk.stem.porter import PorterStemmer\n",
        "# porter_stemmer  = PorterStemmer()\n",
        "# text = \"studies studying cries cry\"\n",
        "# tokenization = nltk.word_tokenize(text)\n",
        "# for w in tokenization:\n",
        "#   print(\"Stemming for {} is {}\".format(w,porter_stemmer.stem(w))) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYhVvJEkRXK4",
        "outputId": "242a0af2-4b34-4f0c-d289-de49275b56c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming for studies is studi\n",
            "Stemming for studying is studi\n",
            "Stemming for cries is cri\n",
            "Stemming for cry is cri\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import nltk\n",
        "# from nltk.stem import \tWordNetLemmatizer\n",
        "# wordnet_lemmatizer = WordNetLemmatizer()\n",
        "# text = \"studies studying cries cry\"\n",
        "# tokenization = nltk.word_tokenize(text)\n",
        "# for w in tokenization:\n",
        "# \tprint(\"Lemma for {} is {}\".format(w, wordnet_lemmatizer.lemmatize(w)))  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeVTlK8gRphT",
        "outputId": "cc34aa4d-64dc-4db5-9529-274f41fed5e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemma for studies is study\n",
            "Lemma for studying is studying\n",
            "Lemma for cries is cry\n",
            "Lemma for cry is cry\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we will define a function for greeting by a bot. bot should give us greeting message\n",
        "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\", \"hey\",)\n",
        "GREETING_RESPONSES = (\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad ! you are talking to me\")\n",
        "def greeting(sentence):\n",
        "  for word in sentence.split():\n",
        "    if word.lower() in GREETING_INPUTS:\n",
        "      return random.choice(GREETING_RESPONSES)"
      ],
      "metadata": {
        "id": "ap-ur1WfPvhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF IDF approach \n",
        "# TF - term frequency - is scoring of the frequency of a word in a current document\n",
        "# TF = (number of times term t in a document)/ (total number of terms in a document)\n",
        "\n",
        "# IDF - inverse document frequency - how rare the word is across document\n",
        "# IDF = 1+log(N/n) where N is the number of documents, n is number of documents a term  t has appeared"
      ],
      "metadata": {
        "id": "c2ER7DE8TQ9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def response(user_response):\n",
        "  robo_response = ''\n",
        "  sent_tokens.append(user_response)\n",
        "  TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words = 'english')\n",
        "  tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "  vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "  idx = vals.argsort()[0][-2]\n",
        "  flat = vals.flatten()\n",
        "  flat.sort()\n",
        "  req_tfidf = flat[-2]\n",
        "  if req_tfidf == 0:\n",
        "    robo_response = robo_response+\"i am sorry I dont know the answer I dont understand what you are saying\"\n",
        "    return robo_response\n",
        "  else:\n",
        "    robo_response = robo_response + sent_tokens[idx]\n",
        "    return robo_response"
      ],
      "metadata": {
        "id": "ytiSHevST7kA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}